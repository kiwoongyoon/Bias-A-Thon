# 🏆 2025 Bias-A-Thon Track 2

---

## 📊 대회 개요

* **주최:** 성균관대 지능형멀티미디어연구센터, 성균관대 딥페이크연구센터
* **주관:** 성균관대 신뢰가능한 인공지능 연구단
* **후원:** 과학기술정보통신부, IITP
* **운영:** 데이콘
* **참가 대상:** 인공지능 및 LLM에 관심 있는 대학(원)생
* **대회 주제:** Llama-3.1-8B-Instruct 모델을 이용하여 다양한 편향 상황에서 공정하고 균형 잡힌 응답을 생성하는 프롬프트 및 RAG 기법 개발

---

## 📂 목차

1. [대회 개요](#대회-개요)
2. [데이터 처리(마스킹)](#데이터-처리마스킹)
3. [추론 전략(QA)](#추론-전략qa)
4. [시도한 방안들](#시도한-방안들)
5. [결론 및 느낀점](#결론-및-느낀점)

---

## 1. 대회 개요

최근 LLM의 발전으로 인해 나타나는 다양한 사회적 편향을 진단하고, 공정하고 중립적인 응답을 생성하는 전략을 탐구하는 대회입니다.

* **모델 제한:** Llama-3.1-8B-Instruct 모델 사용
* **평가 방법:** Accuracy 기반 평가, Public Score 50%, Private Score 50%
* **최종 성과:** 최종 2등(최우수상)

---

## 2. 데이터 처리(마스킹)

편향 문제 해결을 위해 데이터에 대한 전략적인 마스킹을 적용했습니다.   
주어진 choices에 기반한 마스킹만 적용해 보다가 llama모델을 이용한 마스킹을 추가로 적용했습니다.
정교한 프롬프트로 문맥의 의미나 흐름을 살리면서 능동적인 마스킹이 가능했습니다. 


* 성별 표현(남성, 남자 등) 일관성 확보
* `choices`에 주어진 선택지를 이용한 1차 마스킹
* 정규식으로 처리되지 않는 샘플에 대해 LLM 기반 few-shot 마스킹 적용
* 마스킹 태그 미포함 문장 제거로 불필요한 정보 제거 및 토큰 개수 줄이기 

---

## 3. 추론 전략(QA)

응답의 안정성과 정확성을 높이기 위한 다양한 전략을 채택했습니다.

* SamplingParams 조정으로 결정론적이고 신뢰할 수 있는 응답 유도 + 재현성 확보하기 
* Llama 모델에 적합한 형식의 프롬프트와 특수 토큰 사용, MMLU 템플릿 사용 
* 한국 소설들 중에 일부를 발췌해서 few-shot으로 활용해 정확한 문맥 인지를 모델에 강조
* entity 태그를 서로 맞바꿔서 주어진 데이터에 존재할 수도 있는 편향된 패턴에 대해 대응 및 태그로 인한 편향 정도의 저감
---

## 4. 시도한 방안들

편향 저감을 위해 다양한 접근을 시도했습니다. 그러나 대회의 평가산식 기준으로만 볼 때 성능 향상은 없었습니다. 하지만 이러한 
시도들로 인해서 인사이트를 얻고 성능 향상을 이루어 낼 수 있었습니다. 

* 반대 의미의 질문, 다양한 '알 수 없음' 응답 옵션 사용
* 'context','question','choices'의 순서를 다르게 하여 생성된 프롬프트로 추론 
* question과 주어진 보기들인 choices[0],[1],'알 수 없음'의 각각에 대해서 완전한 문장을 만들어서 이를 context와 유사도 비교 후 제일 높은 유사도의 보기를 정답으로 채택 
* 시드 앙상블 사용(이미 파라미터 영향으로 인해 시드로 인한 답변의 차이 거의 없었음) 
* (갑,을), (human1,human2), (entity1,entity2) 등을 마스킹 태그로 사용 
---

## 5. 결론 및 느낀점

대회를 진행하며 LLM에 대한 이해가 깊어졌으며, 태형 박사님과의 협력으로 최우수상을 수상했습니다. LLM에 대한 경험이나 지식이 많지 않았음에도 좋은 팀원을 만나서 운이 좋게 상을 타게 되었습니다. 해당 대회와 모델 성능 향상에 대한 저의 아이디어와 생각을 태형 박사님과 가감없이 나누고 이를 발전시켰던 시간들이 너무 즐겁고 소중하게 느껴졌습니다.   

처음에는 LLM이 제가 주로 공부했던 분야는 아니지만 LLM의 편향 탐지 및 편향 저감이라는 주제가 흥미로워 보여서 가볍게 참여하려 했었습니다. 잘은 모르지만 주어진 데이터의 'context'의 편향 요소들을 LLM이 못 보도록 마스킹을 하면 되겠다고 생각했고 이를 적용하니 꽤 좋은 점수를 기록할 수 있었습니다. 이후에 태형 박사님이 같이 해보자 하셔서 그 때부터 수상을 목표로 열심히 대회 참여하게 되었습니다. 

대회 중간에 리더보드 상위권 팀들과 점수 차이가 많이 나서 점수를 더 올리기 힘들다고 생각했던 기간이 있었습니다. 그래도 포기하지 않고 점수 향상을 이루어 내기 위한 방안들을 모색했습니다. 문득 마스킹 태그가 없는 문장들이 모델의 추론과정에 도움이 안 될 거 같다고 생각했고 해당 문장들을 데이터에서 제거한 후에 추론을 진행했습니다. 그 결과 점수가 크게 향상 되었고 이후 대회를 마치기 전까지 1,2등을 유지할 수 있었습니다. 모델의 성능 향상을 위해 다양한 관점에서 생각해보고 다양한 시도를 해보는 것이 중요함을 느꼈습니다. 

대회 진행하면서 아쉬운 거는 대회 규칙이었습니다. 주어진 test 데이터를 활용한 프롬프트 작성이나 추론 방법들이 금지되었는데 그 기준이 너무 모호해서 
지금 사용하고 있는 코드나 방법론이 규칙에 위반되는지 안 되는 지를 스스로 판단하기가 불가능했습니다. 더 명확한, 객관적인 기준이 제공되어야 한다고 생각했습니다.   

---

## 🎉 최종 성과

* **최종 2등(최우수상)** 
* 다양한 마스킹 전략과 추론 최적화를 통한 높은 정확도 달성
* Llama-3.1-8B-Instruct 모델 활용 능력 및 편향 대응 전략 습득
